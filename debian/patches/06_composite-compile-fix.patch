diff -Nur gimp-2.2.11/app/composite/gimp-composite-generic.c gimp-2.2.11.new/app/composite/gimp-composite-generic.c
--- gimp-2.2.11/app/composite/gimp-composite-generic.c	2006-06-20 10:12:36.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-generic.c	2006-06-20 10:12:37.000000000 -0400
@@ -69,6 +69,7 @@
 
 static guchar add_lut[511];
 static gint32 random_table[RANDOM_TABLE_SIZE];
+static guchar burn_lut[256][256];
 
 /*
  *
@@ -873,18 +874,10 @@
   const guint alpha = (has_alpha1 || has_alpha2) ? MAX(bytes1, bytes2) - 1 : bytes1;
   guint b;
 
-  /* FIXME: Is the burn effect supposed to be dependant on the sign of this
-   * temporary variable? */
-  gint tmp;
-
   while (length--)
     {
       for (b = 0; b < alpha; b++)
-        {
-          tmp = (255 - src1[b]) << 8;
-          tmp /= src2[b] + 1;
-          dest[b] = (guchar) CLAMP(255 - tmp, 0, 255);
-        }
+        dest[b] = burn_lut[src1[b]][src2[b]];
       if (has_alpha1 && has_alpha2)
         dest[alpha] = MIN(src1[alpha], src2[alpha]);
       else if (has_alpha2)
@@ -1444,6 +1437,7 @@
 {
   GRand *gr;
   guint  i;
+  gint   a, b;
 
 #define RANDOM_SEED 314159265
 
@@ -1455,11 +1449,28 @@
 
   g_rand_free (gr);
 
+  /*  generate a table for burn compositing */
+  for (a = 0; a < 256; a++)
+    for (b = 0; b < 256; b++)
+      {
+        /* FIXME: Is the burn effect supposed to be dependant on the sign
+         * of this temporary variable?
+         */
+        gint tmp;
+
+        tmp = (255 - a) << 8;
+        tmp /= b + 1;
+        tmp = (255 - tmp);
+
+        burn_lut[a][b] = CLAMP (tmp, 0, 255);
+      }
+
+  /*  generate a table for saturate add */
   for (i = 0; i < 256; i++)
     add_lut[i] = i;
 
   for (i = 256; i <= 510; i++)
     add_lut[i] = 255;
 
-  return (TRUE);
+  return TRUE;
 }
diff -Nur gimp-2.2.11/app/composite/gimp-composite-mmx.c gimp-2.2.11.new/app/composite/gimp-composite-mmx.c
--- gimp-2.2.11/app/composite/gimp-composite-mmx.c	2006-06-20 10:12:35.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-mmx.c	2006-06-20 10:12:37.000000000 -0400
@@ -136,123 +136,6 @@
 }
 
 void
-gimp_composite_burn_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *_op)
-{
-  uint64 *d = (uint64 *) _op->D;
-  uint64 *a = (uint64 *) _op->A;
-  uint64 *b = (uint64 *) _op->B;
-  gulong n_pixels = _op->n_pixels;
-
-  for (; n_pixels >= 2; n_pixels -= 2)
-    {
-      asm volatile ("  movq         %1,%%mm0\n"
-                    "\tmovq         %2,%%mm1\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq         %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw   %%mm6,%%mm4\n"
-                    "\tpsubusw   %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq         %6,%%mm7\n" /* mm6 = rgba8_alpha_mask_64 */
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-                    
-                    "\tmovq      %%mm7,%0\n"
-                    : "=m" (*d)
-                    : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-      d++;
-      b++;
-      a++;
-    }
-
-  if (n_pixels > 0)
-    {
-      asm volatile ("  movd         %1,%%mm0\n"
-                    "\tmovd         %2,%%mm1\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq         %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw   %%mm6,%%mm4\n"
-                    "\tpsubusw   %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq         %6,%%mm7\n"
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                    "\tmovd      %%mm7,%0\n"
-                    : "=m" (*d)
-                    : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-    }
-
-  asm("emms");
-}
-
-
-void
 gimp_composite_darken_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *_op)
 {
   uint64 *d = (uint64 *) _op->D;
@@ -1376,123 +1259,6 @@
 
 #if 0
 void
-gimp_composite_burn_va8_va8_va8_mmx (GimpCompositeContext *_op)
-{
-  GimpCompositeContext op = *_op;
-
-  asm("movq   %0,%%mm1"
-      :
-      : "m" (*va8_alpha_mask)
-      : "%mm1");
-
-  for (; op.n_pixels >= 4; op.n_pixels -= 4)
-    {
-    asm volatile ("  movq         %0,%%mm0\n"
-                  "\tmovq         %1,%%mm1\n"
-
-                  "\tmovq         %3,%%mm2\n"
-                  "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                  "\tpxor      %%mm4,%%mm4\n"
-                  "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                  "\tmovq      %%mm1,%%mm3\n"
-                  "\tpxor      %%mm5,%%mm5\n"
-                  "\tpunpcklbw %%mm5,%%mm3\n"
-                  "\tmovq         %4,%%mm5\n"
-                  "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                  "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                  "\tmovq         %3,%%mm2\n"
-                  "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                  "\tpxor      %%mm4,%%mm4\n"
-                  "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                  "\tmovq      %%mm1,%%mm3\n"
-                  "\tpxor      %%mm5,%%mm5\n"
-                  "\tpunpckhbw %%mm5,%%mm3\n"
-                  "\tmovq         %4,%%mm5\n"
-                  "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                  "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                  "\tmovq         %5,%%mm4\n"
-                  "\tmovq      %%mm4,%%mm5\n"
-                  "\tpsubusw   %%mm6,%%mm4\n"
-                  "\tpsubusw   %%mm7,%%mm5\n"
-
-                  "\tpackuswb  %%mm4,%%mm5\n"
-
-                  "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                  "\tmovq         %6,%%mm7\n"
-                  "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                  "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                  "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                  "\tmovq      %%mm7,%2\n"
-                  : /* empty */
-                  : "+m" (*op.A), "+m" (*op.B), "+m" (*op.D), "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255_64), "m" (*va8_alpha_mask)
-                  : "%mm1", "%mm2", "%mm3", "%mm4");
-      op.A += 8;
-      op.B += 8;
-      op.D += 8;
-  }
-
-  if (op.n_pixels)
-    {
-    asm volatile ("  movd         %0,%%mm0\n"
-                  "\tmovd         %1,%%mm1\n"
-                  "\tmovq         %3,%%mm2\n"
-                  "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                  "\tpxor      %%mm4,%%mm4\n"
-                  "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                  "\tmovq      %%mm1,%%mm3\n"
-                  "\tpxor      %%mm5,%%mm5\n"
-                  "\tpunpcklbw %%mm5,%%mm3\n"
-                  "\tmovq         %4,%%mm5\n"
-                  "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                  "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                  "\tmovq         %3,%%mm2\n"
-                  "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                  "\tpxor      %%mm4,%%mm4\n"
-                  "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                  "\tmovq      %%mm1,%%mm3\n"
-                  "\tpxor      %%mm5,%%mm5\n"
-                  "\tpunpckhbw %%mm5,%%mm3\n"
-                  "\tmovq         %4,%%mm5\n"
-                  "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                  "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                  "\tmovq         %5,%%mm4\n"
-                  "\tmovq      %%mm4,%%mm5\n"
-                  "\tpsubusw   %%mm6,%%mm4\n"
-                  "\tpsubusw   %%mm7,%%mm5\n"
-
-                  "\tpackuswb  %%mm4,%%mm5\n"
-
-                  "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                  "\tmovq         %6,%%mm7\n"
-                  "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                  "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                  "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                  "\tmovd      %%mm7,%2\n"
-                  : /* empty */
-                  : "m" (*op.A), "m" (*op.B), "m" (*op.D), "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255_64), "m" (*va8_alpha_mask)
-                  : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-  }
-
-  asm("emms");
-}
-
-void
 xxxgimp_composite_coloronly_va8_va8_va8_mmx (GimpCompositeContext *_op)
 {
   GimpCompositeContext op = *_op;
diff -Nur gimp-2.2.11/app/composite/gimp-composite-mmx.h gimp-2.2.11.new/app/composite/gimp-composite-mmx.h
--- gimp-2.2.11/app/composite/gimp-composite-mmx.h	2006-06-20 10:12:35.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-mmx.h	2006-06-20 10:12:37.000000000 -0400
@@ -33,7 +33,6 @@
  *
  */
 extern void gimp_composite_addition_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *ctx);
-extern void gimp_composite_burn_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *ctx);
 extern void gimp_composite_coloronly_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *ctx);
 extern void gimp_composite_darken_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *ctx);
 extern void gimp_composite_difference_rgba8_rgba8_rgba8_mmx (GimpCompositeContext *ctx);
diff -Nur gimp-2.2.11/app/composite/gimp-composite-mmx-installer.c gimp-2.2.11.new/app/composite/gimp-composite-mmx-installer.c
--- gimp-2.2.11/app/composite/gimp-composite-mmx-installer.c	2006-04-13 07:20:13.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-mmx-installer.c	2006-06-20 10:13:52.000000000 -0400
@@ -25,7 +25,6 @@
  { GIMP_COMPOSITE_SUBTRACT, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_subtract_rgba8_rgba8_rgba8_mmx }, 
  { GIMP_COMPOSITE_DARKEN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_darken_rgba8_rgba8_rgba8_mmx }, 
  { GIMP_COMPOSITE_LIGHTEN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_lighten_rgba8_rgba8_rgba8_mmx }, 
- { GIMP_COMPOSITE_BURN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_burn_rgba8_rgba8_rgba8_mmx }, 
  { GIMP_COMPOSITE_GRAIN_EXTRACT, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_grain_extract_rgba8_rgba8_rgba8_mmx }, 
  { GIMP_COMPOSITE_GRAIN_MERGE, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_grain_merge_rgba8_rgba8_rgba8_mmx }, 
  { GIMP_COMPOSITE_SWAP, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_swap_rgba8_rgba8_rgba8_mmx }, 
diff -Nur gimp-2.2.11/app/composite/gimp-composite-sse.c gimp-2.2.11.new/app/composite/gimp-composite-sse.c
--- gimp-2.2.11/app/composite/gimp-composite-sse.c	2006-06-20 10:12:35.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-sse.c	2006-06-20 10:12:37.000000000 -0400
@@ -113,124 +113,6 @@
   asm("emms");
 }
 
-
-void
-gimp_composite_burn_rgba8_rgba8_rgba8_sse (GimpCompositeContext *_op)
-{
-  uint64 *d = (uint64 *) _op->D;
-  uint64 *a = (uint64 *) _op->A;
-  uint64 *b = (uint64 *) _op->B;
-  gulong n_pixels = _op->n_pixels;
-
-  for (; n_pixels >= 2; n_pixels -= 2)
-    {
-      asm volatile ("  movq         %1,%%mm0\n"
-                    "\tmovq         %2,%%mm1\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq         %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw   %%mm6,%%mm4\n"
-                    "\tpsubusw   %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq         %6,%%mm7\n" /* mm6 = rgba8_alpha_mask_64 */
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-                    
-                    "\tmovq      %%mm7,%0\n"
-                    : "=m" (*d)
-                    : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-      d++;
-      b++;
-      a++;
-    }
-
-  if (n_pixels > 0)
-    {
-      asm volatile ("  movd         %1,%%mm0\n"
-                    "\tmovd         %2,%%mm1\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq         %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq         %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq         %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw   %%mm6,%%mm4\n"
-                    "\tpsubusw   %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq         %6,%%mm7\n"
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                    "\tmovd      %%mm7,%0\n"
-                    : "=m" (*d)
-                    : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-    }
-
-  asm("emms");
-}
-
-
 void
 gimp_composite_darken_rgba8_rgba8_rgba8_sse (GimpCompositeContext *_op)
 {
@@ -1332,121 +1214,6 @@
 }
 
 void
-xxxgimp_composite_burn_va8_va8_va8_sse (GimpCompositeContext *_op)
-{
-  GimpCompositeContext op = *_op;
-
-  asm("movq   %0,%%mm1"
-      :
-      : "m" (*va8_alpha_mask)
-      : "%mm1");
-
-  for (; op.n_pixels >= 4; op.n_pixels -= 4)
-    {
-      asm volatile ("  movq      (%0),%%mm0; addl $8,%0\n"
-                    "\tmovq      (%1),%%mm1; addl $8,%1\n"
-
-                    "\tmovq      %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq      %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq      %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq      %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq      %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw     %%mm6,%%mm4\n"
-                    "\tpsubusw     %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq      %6,%%mm7\n"
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                    "\tmovq      %%mm7,(%2); addl $8,%2\n"
-                    : "+r" (op.A), "+r" (op.B), "+r" (op.D)
-                    : "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255), "m" (*va8_alpha_mask)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
-    }
-
-  if (op.n_pixels)
-    {
-      asm volatile ("  movd      (%0),%%mm0\n"
-                    "\tmovd      (%1),%%mm1\n"
-
-                    "\tmovq      %3,%%mm2\n"
-                    "\tpsubb     %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpcklbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpcklbw %%mm5,%%mm3\n"
-                    "\tmovq      %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-
-                    "\t" pdivwqX(mm4,mm5,mm7) "\n"
-
-                    "\tmovq      %3,%%mm2\n"
-                    "\tpsubb   %%mm0,%%mm2\n" /* mm2 = 255 - A */
-                    "\tpxor      %%mm4,%%mm4\n"
-                    "\tpunpckhbw %%mm2,%%mm4\n" /* mm4 = (255- A) * 256  */
-
-                    "\tmovq      %%mm1,%%mm3\n"
-                    "\tpxor      %%mm5,%%mm5\n"
-                    "\tpunpckhbw %%mm5,%%mm3\n"
-                    "\tmovq      %4,%%mm5\n"
-                    "\tpaddusw   %%mm3,%%mm5\n" /* mm5 = B + 1 */
-                    "\t" pdivwqX(mm4,mm5,mm6) "\n"
-
-                    "\tmovq      %5,%%mm4\n"
-                    "\tmovq      %%mm4,%%mm5\n"
-                    "\tpsubusw     %%mm6,%%mm4\n"
-                    "\tpsubusw     %%mm7,%%mm5\n"
-
-                    "\tpackuswb  %%mm4,%%mm5\n"
-
-                    "\t" pminub(mm0,mm1,mm3) "\n" /* mm1 = min(mm0,mm1) clobber mm3 */
-
-                    "\tmovq      %6,%%mm7\n"
-                    "\tpand      %%mm7,%%mm1\n" /* mm1 = mm7 & alpha_mask */
-
-                    "\tpandn     %%mm5,%%mm7\n" /* mm7 = ~mm7 & mm5 */
-                    "\tpor       %%mm1,%%mm7\n" /* mm7 = mm7 | mm1 */
-
-                    "\tmovd      %%mm7,(%2)\n"
-                    : /* empty */
-                    : "r" (op.A), "r" (op.B), "r" (op.D), "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255), "m" (*va8_alpha_mask)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
-    }
-
-  asm("emms");
-}
-
-void
 xxxgimp_composite_coloronly_va8_va8_va8_sse (GimpCompositeContext *_op)
 {
   GimpCompositeContext op = *_op;
diff -Nur gimp-2.2.11/app/composite/gimp-composite-sse.h gimp-2.2.11.new/app/composite/gimp-composite-sse.h
--- gimp-2.2.11/app/composite/gimp-composite-sse.h	2006-06-20 10:12:35.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-sse.h	2006-06-20 10:12:37.000000000 -0400
@@ -26,7 +26,6 @@
  *
  */
 extern void gimp_composite_addition_rgba8_rgba8_rgba8_sse (GimpCompositeContext *ctx);
-extern void gimp_composite_burn_rgba8_rgba8_rgba8_sse (GimpCompositeContext *ctx);
 extern void gimp_composite_coloronly_rgba8_rgba8_rgba8_sse (GimpCompositeContext *ctx);
 extern void gimp_composite_darken_rgba8_rgba8_rgba8_sse (GimpCompositeContext *ctx);
 extern void gimp_composite_difference_rgba8_rgba8_rgba8_sse (GimpCompositeContext *ctx);
diff -Nur gimp-2.2.11/app/composite/gimp-composite-sse-installer.c gimp-2.2.11.new/app/composite/gimp-composite-sse-installer.c
--- gimp-2.2.11/app/composite/gimp-composite-sse-installer.c	2005-08-15 06:07:03.000000000 -0400
+++ gimp-2.2.11.new/app/composite/gimp-composite-sse-installer.c	2006-06-20 10:14:12.000000000 -0400
@@ -25,7 +25,6 @@
  { GIMP_COMPOSITE_SUBTRACT, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_subtract_rgba8_rgba8_rgba8_sse }, 
  { GIMP_COMPOSITE_DARKEN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_darken_rgba8_rgba8_rgba8_sse }, 
  { GIMP_COMPOSITE_LIGHTEN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_lighten_rgba8_rgba8_rgba8_sse }, 
- { GIMP_COMPOSITE_BURN, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_burn_rgba8_rgba8_rgba8_sse }, 
  { GIMP_COMPOSITE_GRAIN_EXTRACT, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_grain_extract_rgba8_rgba8_rgba8_sse }, 
  { GIMP_COMPOSITE_GRAIN_MERGE, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_grain_merge_rgba8_rgba8_rgba8_sse }, 
  { GIMP_COMPOSITE_SWAP, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, GIMP_PIXELFORMAT_RGBA8, gimp_composite_swap_rgba8_rgba8_rgba8_sse }, 
Binary files gimp-2.2.11/app/composite/ns.pyc and gimp-2.2.11.new/app/composite/ns.pyc differ
